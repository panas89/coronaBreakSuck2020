{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "- To prepare a mini version of the data sources for extracting the affliation of the papers. That is, the origin of the papers' data, which we assume the data is collected from the regions/countries that the authors are from.\n",
    "- To identify the countries of the data origin for each paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get only the location information of each papers\n",
    "- None of the columns are unique id, including cord_uid. But, we will use this for now because it has the most unique ids. \n",
    "- The \"merge_raw_data.csv\" is created by Panayiotis. Since it is very large, we want to perform this section step locally to create a mini_file which contains the cord_uid and the affliations information\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # load the data columns and remove some reducdant information\n",
    "# col_to_remove = ['text', 'bibliography']\n",
    "# col_names = pd.read_csv('data/merged_raw_data.csv', nrows=1).columns.tolist()\n",
    "# for c in col_to_remove:\n",
    "#     col_names.remove(c)\n",
    "# col_names = ['cord_uid', 'affiliations', 'location']\n",
    "\n",
    "# # load the data\n",
    "# df = pd.read_csv('data/merged_raw_data.csv', usecols=col_names)\n",
    "\n",
    "# save\n",
    "# df.to_csv('../data_location/merged_raw_data_location.csv')\n",
    "\n",
    "# # check\n",
    "# print(df.shape)\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data and then identify their location\n",
    "\n",
    "Methodology\n",
    "- geograpy\n",
    "    - geograpy1 or 2 did not work because of the errors in installation: https://github.com/Corollarium/geograpy2 has installation issues. DID NOT USE IT\n",
    "    - geograpy3 works: https://github.com/jmbielec/geograpy3\n",
    "- can GeoText as well: https://github.com/elyase/geotext\n",
    "- look like the geograpy3 is more robust to bad text (although not 100% correct) that can detect more regions, but the precision may be low\n",
    "- may be we should combine both the use of geograpy and geotext\n",
    "\n",
    "Data\n",
    "- GeoText country abbreivation to country full name\n",
    "    - https://github.com/elyase/geotext/blob/master/geotext/data/countryInfo.txt\n",
    "    - Need to remove the \"#\" from the header column. Created a countryInfo_revised.txt and save into the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geotext \n",
    "- extract the Genotext abbreivation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # load Geotext location abbreivation\n",
    "# df_geotext_loc = pd.read_csv('../data/data_location/countryInfo_revised.txt', \n",
    "#                                 comment='#', sep='\\t')\n",
    "# # create the dict\n",
    "# geotext_loc_dict = dict(zip(df_geotext_loc['ISO'].tolist(),\n",
    "#                            df_geotext_loc['Country'].tolist()\n",
    "#                            ))\n",
    "\n",
    "# # check\n",
    "# print(geotext_loc_dict)\n",
    "# df_geotext_loc.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "- Two version. One is no paralization, one is. If we run data_utils.extract_location(text, is_robust=True), highly recommend using the parallization version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from covid.data import utils as data_utils\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18558, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gi6uaa83</td>\n",
       "      <td>Todd R Disotell (New York University ; 25 Wave...</td>\n",
       "      <td>25 Waverly Place ; 10003 ; New York ; NY ; USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1wswi7us</td>\n",
       "      <td>Yee Leng Yap ; Xue Wu Zhang ; Antoine Danchin ...</td>\n",
       "      <td>;   ; 75724, Cedex 15 ; Paris ; France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yy96yeu9</td>\n",
       "      <td>David Wang (University of California San Franc...</td>\n",
       "      <td>San Francisco ; California ; United States of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       affiliations  \\\n",
       "0  gi6uaa83  Todd R Disotell (New York University ; 25 Wave...   \n",
       "1  1wswi7us  Yee Leng Yap ; Xue Wu Zhang ; Antoine Danchin ...   \n",
       "2  yy96yeu9  David Wang (University of California San Franc...   \n",
       "\n",
       "                                            location  \n",
       "0     25 Waverly Place ; 10003 ; New York ; NY ; USA  \n",
       "1             ;   ; 75724, Cedex 15 ; Paris ; France  \n",
       "2  San Francisco ; California ; United States of ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv('../data/data_location/merged_raw_data_location.csv')\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# check\n",
    "print(df.shape)\n",
    "df.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cols = ['affiliations', 'location']\n",
    "\n",
    "# # loop through each column's address and make classification\n",
    "# for col in cols:\n",
    "#     preds = []\n",
    "#     for i in tqdm(range(0, df.shape[0])):\n",
    "#         text = df.iloc[i][col]\n",
    "#         # print(text)\n",
    "\n",
    "#         # classification\n",
    "#         countries = data_utils.extract_location(text, is_robust=True)\n",
    "#         preds.append(countries)\n",
    "        \n",
    "#     # assign\n",
    "#     df['%s_country'%col] = preds\n",
    "\n",
    "# # save\n",
    "# df.to_csv('../data/data_location/merged_raw_data_location_identified.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parallelize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affiliations\n"
     ]
    }
   ],
   "source": [
    "cols = ['affiliations', 'location']\n",
    "#num_cores = multiprocessing.cpu_count()\n",
    "num_cores = 12\n",
    "\n",
    "# loop through each column's address and make classification\n",
    "for col in cols:\n",
    "    print(col)\n",
    "    preds = Parallel(n_jobs=num_cores)(delayed(data_utils.extract_location)(i) for i in df[col].tolist())\n",
    "\n",
    "    # assign\n",
    "    df['%s_country'%col] = preds\n",
    "\n",
    "# save\n",
    "df.to_csv('../data/data_location/merged_raw_data_location_identified.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
